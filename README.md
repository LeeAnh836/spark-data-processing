# Spark API - Data Processing Projects

Bá»™ tÃ i liá»‡u táº­p thá»±c hÃ nh xá»­ lÃ½ dá»¯ liá»‡u lá»›n sá»­ dá»¥ng Apache Spark trÃªn ná»n táº£ng Databricks. CÃ¡c bÃ i táº­p Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giÃºp há»c viÃªn náº¯m vá»¯ng cÃ¡c ká»¹ nÄƒng cá»‘t lÃµi trong xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n tÃ¡n.

---

## ğŸ“‹ Danh sÃ¡ch cÃ¡c File

### 1. **TH_1_preview.ipynb** 
**Chá»§ Ä‘á»:** Scala Collection Operations
- **Ná»™i dung:** Giá»›i thiá»‡u cÆ¡ báº£n vá» xá»­ lÃ½ dá»¯ liá»‡u vá»›i Scala
- **BÃ i táº­p:** 
  - BÃ¬nh phÆ°Æ¡ng pháº§n tá»­ trong danh sÃ¡ch (map)
  - Lá»c sá»‘ láº» vÃ  láº­p phÆ°Æ¡ng (filter, map)
  - TÃ­nh toÃ¡n giÃ¡ trá»‹ trung bÃ¬nh, trung vá»‹
  - TÃ¬m sá»‘ nguyÃªn lá»›n thá»© 2
  - TÃ­nh Ä‘á»™ dÃ i tá»« trong danh sÃ¡ch chuá»—i
- **Ká»¹ nÄƒng:** map, filter, sorted, groupBy, maxBy
- **Output:** CÃ¡c giÃ¡ trá»‹ sá»‘ vÃ  danh sÃ¡ch Tuple

---

### 2. **TH2_upload_DBFS.ipynb** 
**Chá»§ Ä‘á»:** Text Processing & Word Frequency Analysis
- **Ná»™i dung:** Xá»­ lÃ½ dá»¯ liá»‡u vÄƒn báº£n tá»« DBFS (Databricks File System)
- **BÃ i táº­p:**
  - Äáº¿m sá»‘ tá»« trong tá»«ng Ä‘oáº¡n vÄƒn
  - Loáº¡i bá» cÃ¡c tá»« phá»• biáº¿n (stopwords)
  - TÃ­nh táº§n suáº¥t tá»«, xÃ¡c Ä‘á»‹nh top 10 tá»« phá»• biáº¿n nháº¥t
  - TÃ¬m tá»« cÃ³ táº§n suáº¥t cao nháº¥t trong má»—i Ä‘oáº¡n vÄƒn
- **Ká»¹ nÄƒng:** Text parsing, groupBy, frequency analysis, sorting
- **Data Source:** Text file tá»« DBFS

---

### 3. **TH3_Spark_Dataframe.ipynb** 
**Chá»§ Ä‘á»:** Spark DataFrame Operations vá»›i JSON Data
- **Ná»™i dung:** LÃ m viá»‡c vá»›i dá»¯ liá»‡u JSON vÃ  thao tÃ¡c DataFrame cÆ¡ báº£n
- **BÃ i táº­p:**
  - Äá»c file JSON (1M.json) tá»« DBFS
  - Lá»c báº£n ghi cÃ³ speed > 0
  - Äáº¿m sá»‘ ProviderId duy nháº¥t
  - TrÃ­ch xuáº¥t biá»ƒn sá»‘ xe tá»‰nh vÃ  thá»‘ng kÃª sá»‘ lÆ°á»£ng
  - Gá»i API Nominatim Ä‘á»ƒ láº¥y tÃªn Ä‘á»‹a Ä‘iá»ƒm tá»« tá»a Ä‘á»™ GPS
- **Ká»¹ nÄƒng:** read JSON, filter, count, groupBy, API integration
- **Data Source:** 1M.json (file dá»¯ liá»‡u xe/GPS)

---

### 4. **TH4_Read_CSV.ipynb** 
**Chá»§ Ä‘á»:** Reading CSV Files with Custom Schema
- **Ná»™i dung:** Äá»c file CSV vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh vÃ  Ä‘á»‹nh nghÄ©a schema
- **BÃ i táº­p:**
  - Äá»c file CSV vá»›i delimiter `;` (thay vÃ¬ dáº¥u pháº©y máº·c Ä‘á»‹nh)
  - Äá»‹nh nghÄ©a custom schema cho 21 cá»™t dá»¯ liá»‡u
  - Kiá»ƒm tra vÃ  xÃ¡c nháº­n kiá»ƒu dá»¯ liá»‡u
- **Ká»¹ nÄƒng:** spark.read.csv, custom delimiter, StructType, schema definition
- **Data Source:** full.csv (bank marketing dataset)
- **Columns:** age, job, marital, education, default, housing, loan, contact, month, dayOfWeek, duration, campaign, pdays, previous, poutcome, emVarRate, consPriceIdx, consConfIdx, euribor3m, nrEmployed, y

---

### 5. **TH5_Spark_Dataframe.ipynb** 
**Chá»§ Ä‘á»:** Advanced DataFrame Analytics - NYC Taxi Data
- **Ná»™i dung:** PhÃ¢n tÃ­ch dá»¯ liá»‡u chuyáº¿n Ä‘i taxi táº¡i New York tá»« báº£ng tÃ­ch há»£p Databricks
- **BÃ i táº­p:**
  - TÃ­nh tiá»n trung bÃ¬nh cho chuyáº¿n Ä‘i > 2 miles
  - TÃ­nh thá»i gian trung bÃ¬nh chuyáº¿n Ä‘i
  - TÃ¬m cáº·p zip code cÃ³ quÃ£ng Ä‘Æ°á»ng trung bÃ¬nh cao nháº¥t
  - Hiá»ƒn thá»‹ top 3 chuyáº¿n Ä‘i Ä‘áº¯t tiá»n theo tá»«ng cáº·p zip code
  - TÃ­nh doanh thu theo ngÃ y trong tuáº§n
  - Join dá»¯ liá»‡u CSV tÃªn quáº­n vá»›i báº£ng trips
  - TÃ­nh doanh thu theo quáº­n (borough)
- **Ká»¹ nÄƒng:** aggregation, join, window function, groupBy, sorting
- **Data Source:** samples.nyctaxi.trips (Databricks sample table)

---

### 6. **TH7_WriteFile.ipynb** 
**Chá»§ Ä‘á»:** ETL Pipeline - Data Cleaning & Writing Files
- **Ná»™i dung:** Thiáº¿t káº¿ luá»“ng ETL Ä‘á»ƒ lÃ m sáº¡ch dá»¯ liá»‡u nhÃ¢n viÃªn vÃ  ghi ra cÃ¡c Ä‘á»‹nh dáº¡ng khÃ¡c nhau
- **BÃ i táº­p:**
  - Äá»c file nhÃ¢n viÃªn tá»« DBFS
  - Thá»‘ng kÃª Ä‘á»™ tuá»•i nhÃ¢n viÃªn (tÃ­nh tá»« ngÃ y sinh)
  - Táº¡o cá»™t fullName, chuáº©n hÃ³a kiá»ƒu chá»¯, loáº¡i bá» trÃ¹ng láº·p
  - Chuáº©n hÃ³a dá»¯ liá»‡u SSN (sá»‘ báº£o hiá»ƒm xÃ£ há»™i)
  - Ghi dá»¯ liá»‡u ra multiple format (CSV, Parquet, JSON) vá»›i compression
- **Ká»¹ nÄƒng:** data cleaning, deduplication, string manipulation, write with different formats/compression
- **Output Formats:** CSV, Parquet (snappy), JSON (gzip)
- **Data Source:** people-with-dups.txt

---

### 7. **TH8_Review.ipynb** 
**Chá»§ Ä‘á»:** Comprehensive Review - Complex Data Aggregation
- **Ná»™i dung:** BÃ i táº­p tá»•ng há»£p ká»¹ nÄƒng vá»›i dá»¯ liá»‡u sá»± kiá»‡n e-commerce
- **BÃ i táº­p:**
  - Äá»c file JSON tá»« GitHub vá»›i schema phá»©c táº¡p (nested structures)
  - TÃ­nh trung bÃ¬nh doanh thu (purchase_revenue) theo user_id
  - CÃ¡c bÃ i táº­p tiáº¿p theo chÆ°a Ä‘Æ°á»£c mÃ´ táº£ Ä‘áº§y Ä‘á»§
- **Ká»¹ nÄƒng:** complex schema parsing, nested structures, groupBy aggregation, window functions
- **Data Source:** Events data tá»« GitHub (e-commerce transactions)
- **Schema:** Includes device, ecommerce (struct), items (array), geo (struct)

---

---

## ğŸ¯ Ká»¹ nÄƒng ChÃ­nh Ä‘Æ°á»£c Há»c

- âœ¨ **Scala Collections:** map, filter, sorted, groupBy, maxBy, distinct
- âœ¨ **Spark DataFrame:** read, select, filter, groupBy, join, aggregation
- âœ¨ **Data Formats:** JSON, CSV, Parquet, TXT
- âœ¨ **Data Processing:** text parsing, frequency analysis, deduplication, normalization
- âœ¨ **ETL Concepts:** data cleaning, transformation, loading, multiple output formats
- âœ¨ **File Systems:** DBFS, local file systems
- âœ¨ **Advanced Features:** nested structures, window functions, API integration

---

## ğŸ“ LÆ°u Ã½

- CÃ¡c file Scala (.ipynb) cháº¡y trÃªn Databricks platform
- Cáº§n báº­t cluster Databricks Ä‘á»ƒ truy cáº­p dá»¯ liá»‡u sample tables
- Má»™t sá»‘ file cáº§n táº£i dá»¯ liá»‡u tá»« Google Classroom
- File TH8_Review táº£i dá»¯ liá»‡u Ä‘á»™ng tá»« GitHub API
